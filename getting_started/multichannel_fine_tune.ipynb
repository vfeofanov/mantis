{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "demonstration data set from the UEA collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dims:  (160, 10, 400)\n",
      "X_test dims:  (74, 10, 400)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = [np.load(f'../data/HandMovementDirection/{variable}_{set_name}.npy')\n",
    "        for variable in ['X', 'y'] for set_name in ['train', 'test']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = data\n",
    "\n",
    "print(\"X_train dims: \", X_train.shape)\n",
    "print(\"X_test dims: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to apply the foundation model to your data, `X_train` and `X_test` should be of the shape `(n_samples, n_channels=1, seq_len)`, where `seq_len` is a multiple of 32\n",
    "\n",
    "if original sequence length is different, resize it, for example, using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dims:  (160, 10, 512)\n",
      "X_test dims:  (74, 10, 512)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def resize(X):\n",
    "    X_scaled = F.interpolate(torch.tensor(X, dtype=torch.float), size=512, mode='linear', align_corners=False)\n",
    "    return X_scaled.numpy()\n",
    "    \n",
    "X_train, X_test = resize(X_train), resize(X_test)\n",
    "\n",
    "print(\"X_train dims: \", X_train.shape)\n",
    "print(\"X_test dims: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mantis.architecture import Mantis8M\n",
    "    \n",
    "device = 'cpu' # set device\n",
    "network = Mantis8M(device=device) # init model\n",
    "network = network.from_pretrained(\"paris-noah/Mantis-8M\") # load weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning without an adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `adapter=None` for the `.fit()` method, which means that each channel is independently sent to the foundation model during the forward pass.\n",
    "All the outputs are further concatenated in a flattened manner and sent to the classification head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize the trainer and some arguments to pass during fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mantis.trainer import MantisTrainer\n",
    "\n",
    "model = MantisTrainer(device=device, network=network)\n",
    "\n",
    "# initialize some training parameters\n",
    "def init_optimizer(params): return torch.optim.AdamW(\n",
    "    params, lr=2e-4, betas=(0.9, 0.999), weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: Train Loss 0.6181: 100%|████████████████████████████████████████████████████| 100/100 [00:44<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "fine_tuning_type = 'head'\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit(X_train, y_train, num_epochs=100,\n",
    "            fine_tuning_type=fine_tuning_type, init_optimizer=init_optimizer)\n",
    "\n",
    "# predict labels\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set is 0.20270270270270271\n"
     ]
    }
   ],
   "source": [
    "# evaluate the performance\n",
    "print(f'Accuracy on the test set is {np.mean(y_test == y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: Train Loss 0.0108: 100%|████████████████████████████████████████████████████| 100/100 [00:46<00:00,  2.17it/s]\n"
     ]
    }
   ],
   "source": [
    "fine_tuning_type = 'full'\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit(X_train, y_train, num_epochs=100,\n",
    "            fine_tuning_type=fine_tuning_type, init_optimizer=init_optimizer)\n",
    "\n",
    "# predict labels\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set is 0.28378378378378377\n"
     ]
    }
   ],
   "source": [
    "# evaluate the performance\n",
    "print(f'Accuracy on the test set is {np.mean(y_test == y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract deep features with an adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the number of channels is too large and there is no access to multiple or powerful GPUs, the backpropagation step may be too costly.\n",
    "In this case, one can try to reduce dimension first and then send the transformed input to the foundation model.\n",
    "\n",
    "Here, we will use PCA, please refer to `multichannel_adapters.ipynb` for more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_reduced_train dims:  (160, 5, 512)\n",
      "X_reduced_test dims:  (74, 5, 512)\n"
     ]
    }
   ],
   "source": [
    "from mantis.adapters import MultichannelProjector\n",
    "\n",
    "adapter = MultichannelProjector(new_num_channels=5, patch_window_size=1, base_projector='pca')\n",
    "adapter.fit(X_train)\n",
    "X_reduced_train, X_reduced_test = adapter.transform(X_train), adapter.transform(X_test)\n",
    "\n",
    "print(\"X_reduced_train dims: \", X_reduced_train.shape)\n",
    "print(\"X_reduced_test dims: \", X_reduced_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: Train Loss 0.8441: 100%|████████████████████████████████████████████████████| 100/100 [00:22<00:00,  4.47it/s]\n"
     ]
    }
   ],
   "source": [
    "fine_tuning_type = 'head'\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit(X_reduced_train, y_train, num_epochs=100,\n",
    "            fine_tuning_type=fine_tuning_type, init_optimizer=init_optimizer)\n",
    "\n",
    "# predict labels\n",
    "y_pred = model.predict(X_reduced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set is 0.2972972972972973\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance\n",
    "print(f'Accuracy on the test set is {np.mean(y_test == y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: Train Loss 0.0069: 100%|████████████████████████████████████████████████████| 100/100 [00:23<00:00,  4.31it/s]\n"
     ]
    }
   ],
   "source": [
    "fine_tuning_type = 'full'\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit(X_reduced_train, y_train, num_epochs=100,\n",
    "            fine_tuning_type=fine_tuning_type, init_optimizer=init_optimizer)\n",
    "\n",
    "# predict labels\n",
    "y_pred = model.predict(X_reduced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set is 0.3918918918918919\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance\n",
    "print(f'Accuracy on the test set is {np.mean(y_test == y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
